# ğŸš€ Apache Kafka avec Spring Cloud Stream

Ce projet dÃ©montre l'utilisation d'Apache Kafka avec Spring Boot pour le traitement de flux de messages en temps rÃ©el, incluant production, consommation, et traitement avec Kafka Streams.

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [Architecture](#architecture)
- [PrÃ©requis](#prÃ©requis)
- [Partie 1 : Producer & Consumer CLI](#partie-1--producer--consumer-cli)
- [Partie 2 : API REST Producer](#partie-2--api-rest-producer)
- [Partie 3 : Kafka Streams & Analytics](#partie-3--kafka-streams--analytics)
- [Configuration](#configuration)
- [Tests](#tests)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Vue d'ensemble

Ce projet illustre trois aspects fondamentaux de Kafka :

| Partie | Description | Technologies |
|--------|-------------|--------------|
| **Partie 1** | Production et consommation de messages via CLI | Kafka CLI Tools |
| **Partie 2** | Exposition d'une API REST pour publier des messages | Spring Cloud Stream, StreamBridge |
| **Partie 3** | Analyse en temps rÃ©el avec visualisation | Kafka Streams, SSE, Graphiques dynamiques |

### FonctionnalitÃ©s principales

âœ… **Production de messages** via CLI et API REST  
âœ… **Consommation de messages** en temps rÃ©el  
âœ… **Traitement de flux** avec Kafka Streams  
âœ… **Analytics en temps rÃ©el** avec fenÃªtres glissantes  
âœ… **Visualisation dynamique** via Server-Sent Events (SSE)  
âœ… **State Store** pour le comptage des visites  

---

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Frontend (Browser)                    â”‚
â”‚                  Graphiques dynamiques (SSE)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ SSE Stream
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Spring Boot Application                    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ REST         â”‚  â”‚ Kafka        â”‚  â”‚ Kafka        â”‚       â”‚
â”‚  â”‚ Controller   â”‚  â”‚ Producer     â”‚  â”‚ Consumer     â”‚       â”‚
â”‚  â”‚              â”‚  â”‚ (StreamBridgeâ”‚  â”‚              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                  â”‚                â”‚               â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                            â”‚                                â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚                   â”‚ Kafka Streams   â”‚                       â”‚
â”‚                   â”‚ (count-store)   â”‚                       â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Apache Kafka Broker (Docker)                   â”‚
â”‚                                                             â”‚
â”‚  Topics: R2, R4, P1, P2                                     â”‚
â”‚  Port: 9092                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ PrÃ©requis

- **Docker** & **Docker Compose**
- **Java** 17 ou supÃ©rieur
- **Maven** 3.8+
- **Spring Boot** 3.x
- **Apache Kafka** (via Docker)

### DÃ©pendances Maven

```xml
<dependencies>
    <!-- Spring Cloud Stream Kafka -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-stream-binder-kafka-streams</artifactId>
    </dependency>
    
    <!-- Spring Cloud Stream -->
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-stream</artifactId>
    </dependency>
    
    <!-- Kafka Streams -->
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-streams</artifactId>
    </dependency>
    
    <!-- Spring Web -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>
</dependencies>
```

---

## ğŸ“¡ Partie 1 : Producer & Consumer CLI

Cette section couvre les bases de Kafka en utilisant les outils en ligne de commande pour produire et consommer des messages.

### Ã‰tape 1 : VÃ©rifier que Kafka est en cours d'exÃ©cution

```bash
docker ps
```

**RÃ©sultat attendu :**
```
CONTAINER ID   IMAGE                    STATUS       PORTS                    NAMES
abc123def456   confluentinc/cp-kafka    Up 2 hours   0.0.0.0:9092->9092/tcp  bdcc-kafka-broker
```

> âš ï¸ Le conteneur **bdcc-kafka-broker** doit apparaÃ®tre avec le statut `Up`.

### Ã‰tape 2 : CrÃ©er un topic (optionnel)

```bash
docker exec -it bdcc-kafka-broker kafka-topics \
  --create \
  --bootstrap-server broker:9092 \
  --topic R2 \
  --partitions 3 \
  --replication-factor 1
```

### Ã‰tape 3 : Lister les topics existants

```bash
docker exec -it bdcc-kafka-broker kafka-topics \
  --list \
  --bootstrap-server broker:9092
```

### Ã‰tape 4 : Produire des messages (Producer)

Ouvrez un terminal et exÃ©cutez :

```bash
docker exec -it bdcc-kafka-broker kafka-console-producer \
  --broker-list broker:9092 \
  --topic R2
```

Ensuite, tapez des messages (un par ligne) :

```
> Hello Kafka
> Message 1
> Message 2
> Test de production
```

> ğŸ’¡ **Astuce** : Appuyez sur `Ctrl+C` pour quitter le producer.

### Ã‰tape 5 : Consommer des messages (Consumer)

Dans un **second terminal**, lancez le consumer :

```bash
docker exec -it bdcc-kafka-broker kafka-console-consumer \
  --bootstrap-server broker:9092 \
  --topic R2 \
  --from-beginning
```

**RÃ©sultat attendu :**
```
Hello Kafka
Message 1
Message 2
Test de production
```

### Options avancÃ©es du Consumer

#### Afficher les clÃ©s et valeurs

```bash
docker exec -it bdcc-kafka-broker kafka-console-consumer \
  --bootstrap-server broker:9092 \
  --topic R4 \
  --from-beginning \
  --property print.key=true \
  --property print.value=true \
  --property key.separator=" : "
```

#### Consommer avec des dÃ©sÃ©rialiseurs spÃ©cifiques

```bash
docker exec -it bdcc-kafka-broker kafka-console-consumer \
  --bootstrap-server broker:9092 \
  --topic R4 \
  --property print.key=true \
  --property print.value=true \
  --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \
  --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
```

---

## ğŸŒ Partie 2 : API REST Producer

Cette partie expose une API REST permettant de publier des messages dans Kafka via Spring Cloud Stream.

### Architecture

```
Client HTTP
    â†“
REST Controller (/publish/{topic}/{name})
    â†“
StreamBridge.send()
    â†“
Kafka Topic (R4, R2, etc.)
    â†“
Kafka Consumer (CLI ou Application)
```

### ImplÃ©mentation

#### REST Controller

```java
@RestController
@RequestMapping("/api")
public class KafkaProducerController {
    
    private final StreamBridge streamBridge;
    
    public KafkaProducerController(StreamBridge streamBridge) {
        this.streamBridge = streamBridge;
    }
    
    @PostMapping("/publish/{topic}/{name}")
    public ResponseEntity<Map<String, String>> publish(
            @PathVariable String topic,
            @PathVariable String name) {
        
        // CrÃ©ation de l'Ã©vÃ©nement
        PageEvent event = new PageEvent(name, new Date(), System.currentTimeMillis());
        
        // Envoi vers Kafka
        streamBridge.send(topic, event);
        
        return ResponseEntity.ok(Map.of(
            "status", "Message publiÃ© avec succÃ¨s",
            "topic", topic,
            "name", name
        ));
    }
}
```

#### ModÃ¨le de donnÃ©es

```java
@Data
@AllArgsConstructor
@NoArgsConstructor
public class PageEvent {
    private String name;       // Nom de la page (P1, P2, etc.)
    private Date date;         // Date de l'Ã©vÃ©nement
    private Long timestamp;    // Timestamp Unix
}
```

### Configuration application.yml

```yaml
spring:
  cloud:
    stream:
      kafka:
        binder:
          brokers: localhost:9092
    function:
      definition: pageEventConsumer
```

### Tests de l'API

#### 1. Publier un message via cURL

```bash
# Publication sur le topic R4 avec le nom "P1"
curl -X POST http://localhost:8080/api/publish/R4/P1

# Publication sur le topic R4 avec le nom "P2"
curl -X POST http://localhost:8080/api/publish/R4/P2

# Publication multiple
for i in {1..10}; do
  curl -X POST http://localhost:8080/api/publish/R4/P1
  sleep 1
done
```

#### 2. VÃ©rifier la rÃ©ception dans Kafka

Dans un terminal, lancez le consumer :

```bash
docker exec -it bdcc-kafka-broker kafka-console-consumer \
  --bootstrap-server broker:9092 \
  --topic R4 \
  --from-beginning \
  --property print.key=true \
  --property print.value=true
```

**RÃ©sultat attendu :**
```
P1 : {"name":"P1","date":"2024-12-13T10:30:45.123Z","timestamp":1702467045123}
P2 : {"name":"P2","date":"2024-12-13T10:30:46.456Z","timestamp":1702467046456}
P1 : {"name":"P1","date":"2024-12-13T10:30:47.789Z","timestamp":1702467047789}
```

#### 3. Test avec Postman

**MÃ©thode** : `POST`  
**URL** : `http://localhost:8080/api/publish/R4/P1`  
**Headers** : `Content-Type: application/json`

**RÃ©ponse** :
```json
{
  "status": "Message publiÃ© avec succÃ¨s",
  "topic": "R4",
  "name": "P1"
}
```

---

## ğŸ“Š Partie 3 : Kafka Streams & Analytics

Cette partie implÃ©mente un systÃ¨me d'analytics en temps rÃ©el pour compter les visites de pages avec Kafka Streams.

### Objectif

Compter en temps rÃ©el le nombre de visites des pages **P1** et **P2** en utilisant :
- **Kafka Streams** pour le traitement
- **State Store** (`count-store`) pour le stockage des compteurs
- **FenÃªtres glissantes** de 5 secondes
- **Server-Sent Events (SSE)** pour le streaming vers le frontend

### Architecture du traitement

```
Topic: R4 (PageEvent)
    â†“
Kafka Streams Processor
    â†“
FenÃªtre glissante (5 secondes)
    â†“
AgrÃ©gation (count)
    â†“
State Store: count-store
    â†“
SSE Endpoint â†’ Frontend
    â†“
Graphiques en temps rÃ©el
```

### ImplÃ©mentation

#### Configuration Kafka Streams

```java
@Configuration
public class KafkaStreamsConfig {
    
    @Bean
    public Function<KStream<String, PageEvent>, KStream<String, Long>> kStreamFunction() {
        return input -> input
            // Grouper par nom de page
            .groupByKey(Grouped.with(Serdes.String(), new JsonSerde<>(PageEvent.class)))
            
            // FenÃªtre glissante de 5 secondes
            .windowedBy(TimeWindows.of(Duration.ofSeconds(5)))
            
            // Compter les Ã©vÃ©nements
            .count(Materialized.as("count-store"))
            
            // Transformer en KStream
            .toStream()
            
            // Extraire la clÃ© de la fenÃªtre
            .map((windowedKey, count) -> 
                new KeyValue<>(windowedKey.key(), count));
    }
}
```

#### Service d'Analytics

```java
@Service
public class AnalyticsService {
    
    @Autowired
    private StreamsBuilderFactoryBean streamsBuilderFactoryBean;
    
    public Map<String, Long> getPageViewCounts() {
        KafkaStreams kafkaStreams = streamsBuilderFactoryBean.getKafkaStreams();
        
        if (kafkaStreams == null) {
            return Map.of();
        }
        
        ReadOnlyKeyValueStore<String, Long> store = kafkaStreams
            .store(StoreQueryParameters.fromNameAndType(
                "count-store", 
                QueryableStoreTypes.keyValueStore()
            ));
        
        Map<String, Long> counts = new HashMap<>();
        
        try (KeyValueIterator<String, Long> iterator = store.all()) {
            while (iterator.hasNext()) {
                KeyValue<String, Long> entry = iterator.next();
                counts.put(entry.key, entry.value);
            }
        }
        
        return counts;
    }
}
```

#### REST Controller avec SSE

```java
@RestController
@RequestMapping("/api/analytics")
public class AnalyticsController {
    
    @Autowired
    private AnalyticsService analyticsService;
    
    // Endpoint REST classique
    @GetMapping("/page-views")
    public ResponseEntity<Map<String, Long>> getPageViews() {
        return ResponseEntity.ok(analyticsService.getPageViewCounts());
    }
    
    // Endpoint SSE pour streaming en temps rÃ©el
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public Flux<Map<String, Long>> streamPageViews() {
        return Flux.interval(Duration.ofSeconds(1))
            .map(tick -> analyticsService.getPageViewCounts());
    }
}
```

### Configuration complÃ¨te (application.yml)

```yaml
spring:
  application:
    name: kafka-streams-app
    
  cloud:
    stream:
      kafka:
        binder:
          brokers: localhost:9092
          
        streams:
          binder:
            configuration:
              commit.interval.ms: 1000
              default.key.serde: org.apache.kafka.common.serialization.Serdes$StringSerde
              default.value.serde: org.springframework.kafka.support.serializer.JsonSerde
              
      bindings:
        kStreamFunction-in-0:
          destination: R4
          group: analytics-group
          
        kStreamFunction-out-0:
          destination: R4-analytics
          
    function:
      definition: kStreamFunction

server:
  port: 8080
```

### Frontend - Visualisation en temps rÃ©el

#### HTML + JavaScript (Chart.js)

```html
<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <title>Analytics Kafka - Visites en temps rÃ©el</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            padding: 20px;
            background: #f5f5f5;
        }
        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .chart-container {
            position: relative;
            height: 400px;
            margin-top: 30px;
        }
        .stats {
            display: flex;
            justify-content: space-around;
            margin-top: 20px;
        }
        .stat-card {
            background: #4CAF50;
            color: white;
            padding: 20px;
            border-radius: 8px;
            text-align: center;
            flex: 1;
            margin: 0 10px;
        }
        .stat-card h3 {
            margin: 0;
            font-size: 2em;
        }
        .stat-card p {
            margin: 5px 0 0 0;
            opacity: 0.9;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ“Š Analytics Kafka - Visites en temps rÃ©el</h1>
        
        <div class="stats">
            <div class="stat-card">
                <h3 id="p1-count">0</h3>
                <p>Visites P1</p>
            </div>
            <div class="stat-card" style="background: #2196F3;">
                <h3 id="p2-count">0</h3>
                <p>Visites P2</p>
            </div>
            <div class="stat-card" style="background: #FF9800;">
                <h3 id="total-count">0</h3>
                <p>Total</p>
            </div>
        </div>
        
        <div class="chart-container">
            <canvas id="pageViewsChart"></canvas>
        </div>
    </div>

    <script>
        // Configuration du graphique
        const ctx = document.getElementById('pageViewsChart').getContext('2d');
        const chart = new Chart(ctx, {
            type: 'bar',
            data: {
                labels: ['P1', 'P2'],
                datasets: [{
                    label: 'Nombre de visites',
                    data: [0, 0],
                    backgroundColor: [
                        'rgba(76, 175, 80, 0.8)',
                        'rgba(33, 150, 243, 0.8)'
                    ],
                    borderColor: [
                        'rgba(76, 175, 80, 1)',
                        'rgba(33, 150, 243, 1)'
                    ],
                    borderWidth: 2
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    y: {
                        beginAtZero: true,
                        ticks: {
                            stepSize: 1
                        }
                    }
                },
                animation: {
                    duration: 500
                }
            }
        });

        // Connexion SSE
        const eventSource = new EventSource('http://localhost:8080/api/analytics/stream');
        
        eventSource.onmessage = function(event) {
            const data = JSON.parse(event.data);
            
            const p1Count = data.P1 || 0;
            const p2Count = data.P2 || 0;
            const total = p1Count + p2Count;
            
            // Mise Ã  jour des cartes
            document.getElementById('p1-count').textContent = p1Count;
            document.getElementById('p2-count').textContent = p2Count;
            document.getElementById('total-count').textContent = total;
            
            // Mise Ã  jour du graphique
            chart.data.datasets[0].data = [p1Count, p2Count];
            chart.update();
        };
        
        eventSource.onerror = function(error) {
            console.error('Erreur SSE:', error);
        };
    </script>
</body>
</html>
```

### Tests complets

#### 1. Simuler des visites

Script bash pour gÃ©nÃ©rer du trafic :

```bash
#!/bin/bash

echo "ğŸš€ GÃ©nÃ©ration de trafic Kafka..."

# Boucle infinie
while true; do
    # GÃ©nÃ©ration alÃ©atoire de visites P1 ou P2
    PAGE=$((RANDOM % 2 + 1))
    
    curl -X POST "http://localhost:8080/api/publish/R4/P$PAGE" -s > /dev/null
    echo "âœ… Visite P$PAGE envoyÃ©e"
    
    # Pause alÃ©atoire entre 0.5 et 2 secondes
    sleep $((RANDOM % 15 + 5))0.1
done
```

Rendez le script exÃ©cutable et lancez-le :

```bash
chmod +x generate_traffic.sh
./generate_traffic.sh
```

#### 2. VÃ©rifier le State Store

```bash
# Via l'API REST
curl http://localhost:8080/api/analytics/page-views

# RÃ©sultat attendu
{
  "P1": 15,
  "P2": 23
}
```

#### 3. Observer le flux SSE

```bash
curl -N http://localhost:8080/api/analytics/stream
```

**Sortie attendue (stream continu) :**
```
data:{"P1":15,"P2":23}

data:{"P1":16,"P2":23}

data:{"P1":16,"P2":24}
```

---

## âš™ï¸ Configuration

### Topics Kafka utilisÃ©s

| Topic | Description | ClÃ© | Valeur |
|-------|-------------|-----|--------|
| **R2** | Messages simples (CLI) | - | String |
| **R4** | Ã‰vÃ©nements de pages | String (P1, P2) | PageEvent (JSON) |
| **R4-analytics** | RÃ©sultats agrÃ©gÃ©s | String | Long (count) |

### Ports utilisÃ©s

| Service | Port | Description |
|---------|------|-------------|
| **Kafka Broker** | 9092 | Broker Kafka |
| **Zookeeper** | 2181 | Coordination Kafka |
| **Spring Boot App** | 8080 | API REST + SSE |

---

## ğŸ§ª Tests

### Test du Producer CLI

```bash
# Terminal 1 : Producer
docker exec -it bdcc-kafka-broker kafka-console-producer \
  --broker-list broker:9092 \
  --topic R2

# Terminal 2 : Consumer
docker exec -it bdcc-kafka-broker kafka-console-consumer \
  --bootstrap-server broker:9092 \
  --topic R2 \
  --from-beginning
```

### Test de l'API REST

```bash
# Test simple
curl -X POST http://localhost:8080/api/publish/R4/P1

# Test avec boucle
for i in {1..20}; do
  curl -X POST http://localhost:8080/api/publish/R4/P1
  curl -X POST http://localhost:8080/api/publish/R4/P2
  sleep 0.5
done
```

### Test du State Store

```bash
# RÃ©cupÃ©rer les compteurs
curl http://localhost:8080/api/analytics/page-views | jq

# Stream SSE
curl -N http://localhost:8080/api/analytics/stream
```

---

## ğŸ”§ Troubleshooting

### ProblÃ¨me : Kafka ne dÃ©marre pas

```bash
# VÃ©rifier les logs
docker logs bdcc-kafka-broker

# RedÃ©marrer le conteneur
docker restart bdcc-kafka-broker
```

### ProblÃ¨me : Le State Store est vide

```bash
# VÃ©rifier l'Ã©tat de Kafka Streams
curl http://localhost:8080/actuator/health

# RecrÃ©er le State Store
docker exec -it bdcc-kafka-broker kafka-streams-application-reset \
  --application-id kafka-streams-app \
  --bootstrap-servers broker:9092
```

### ProblÃ¨me : Messages non reÃ§us

```bash
# VÃ©rifier les topics
docker exec -it bdcc-kafka-broker kafka-topics \
  --list \
  --bootstrap-server broker:9092

# VÃ©rifier les consumer groups
docker exec -it bdcc-kafka-broker kafka-consumer-groups \
  --list \
  --bootstrap-server broker:9092
```

### ProblÃ¨me : SSE ne se connecte pas

1. VÃ©rifier CORS dans le controller :

```java
@CrossOrigin(origins = "*")
@RestController
@RequestMapping("/api/analytics")
public class AnalyticsController {
    // ...
}
```

2. Tester avec curl :

```bash
curl -N -H "Accept: text/event-stream" \
  http://localhost:8080/api/analytics/stream
```

---

## ğŸ“š Concepts clÃ©s

### FenÃªtres glissantes (Tumbling Windows)

```java
TimeWindows.of(Duration.ofSeconds(5))
```

- CrÃ©e des fenÃªtres de 5 secondes
- Comptage rÃ©initialisÃ© Ã  chaque fenÃªtre
- Permet l'analyse temporelle

### State Store

- Stockage clÃ©-valeur local
- Maintient l'Ã©tat des agrÃ©gations
- Interrogeable via `ReadOnlyKeyValueStore`
- SauvegardÃ© automatiquement

### StreamBridge

- Interface pour publier des messages
- DÃ©couplage du code et de Kafka
- Gestion automatique de la sÃ©rialisation

---

## ğŸš€ AmÃ©liorations futures

- [ ] Ajouter Kafka Connect pour l'intÃ©gration de bases de donnÃ©es
- [ ] ImplÃ©menter des fenÃªtres coulissantes (Sliding Windows)
- [ ] Ajouter l'authentification SASL/SSL
- [ ] CrÃ©er un dashboard React pour les analytics
- [ ] ImplÃ©menter Schema Registry pour la gestion des schÃ©mas
- [ ] Ajouter des alertes en temps rÃ©el
- [ ] ImplÃ©menter KSQL pour des requÃªtes SQL sur les streams
- [ ] Ajouter des tests d'intÃ©gration avec Testcontainers

---

## ğŸ“– Ressources

- [Documentation Kafka](https://kafka.apache.org/documentation/)
- [Spring Cloud Stream](https://spring.io/projects/spring-cloud-stream)
- [Kafka Streams](https://kafka.apache.org/documentation/streams/)
- [Chart.js](https://www.chartjs.org/)

---

## ğŸ“ Licence

Ce projet est Ã  usage Ã©ducatif.

---

## ğŸ‘¥ Contributeurs

DÃ©veloppÃ© dans le cadre d'un projet d'apprentissage d'Apache Kafka et du traitement de flux en temps rÃ©el.
